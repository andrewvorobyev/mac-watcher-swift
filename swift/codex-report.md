# Codex Context Management Report

## Context Pipeline
- The session keeps an in-memory `ConversationHistory` of API-visible response items (messages, tool calls, reasoning), and each turn rebuilds the prompt by concatenating this history with new inputs so the model always sees the full transcript ([conversation_history.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/conversation_history.rs#L3), [codex.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/codex.rs#L666), [codex.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/codex.rs#L976)).
- After every streamed turn, the reported `TokenUsage` is stored and emitted to the UI, giving visibility into current context consumption ([codex.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/codex.rs#L1740), [protocol.rs](https://github.com/openai/codex/blob/main/codex-rs/protocol/src/protocol.rs#L630)).

## Automatic Compaction
- Each completed turn compares the accumulated context tokens against a per-model auto-compaction threshold; exceeding it triggers an inline summarisation turn seeded with the "Start Summarization" input ([codex.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/codex.rs#L1744), [compact.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/codex/compact.rs#L55)).
- The compact turn uses a dedicated prompt and bridge template to replace the transcript with a concise summary plus the original user messages, then records that replacement in the rollout log for future turns ([compact.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/codex/compact.rs#L106), [compact prompt](https://github.com/openai/codex/blob/main/codex-rs/core/templates/compact/prompt.md#L1), [history bridge](https://github.com/openai/codex/blob/main/codex-rs/core/templates/compact/history_bridge.md#L1)).

## Default Limits
- The default model slug is `gpt-5`, which carries a 272 000-token context window and 128 000-token output budget in the baked-in OpenAI metadata table ([config.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/config.rs#L40), [openai_model_info.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/openai_model_info.rs#L30), [openai_model_info.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/openai_model_info.rs#L64)).
- No auto-compaction ceiling is set by default (`model_auto_compact_token_limit: None`), so summarisation only triggers when a user or profile overrides that field ([config.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/config.rs#L68), [config.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/config.rs#L1624), [config.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/config.rs#L1814)).

## Prompt Construction
- Each model family starts from the shared system prompt (`core/prompt.md`) that sets role framing, tone, planning expectations, sandbox rules, and editing constraints; these instructions are embedded into every turn ([model_family.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/model_family.rs#L4), [prompt.md](https://github.com/openai/codex/blob/main/codex-rs/core/prompt.md#L1)).
- Families with special behaviour swap in customised prompts—for example GPT-5 Codex models load `gpt_5_codex_prompt.md`, which tightens CLI, patch, and tool-usage requirements before turns are issued ([model_family.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/model_family.rs#L102), [gpt_5_codex_prompt.md](https://github.com/openai/codex/blob/main/codex-rs/core/gpt_5_codex_prompt.md#L1)).
- Review mode uses a separate reviewer prompt that enforces severity tagging and JSON output when Codex forks into a review task ([client_common.rs](https://github.com/openai/codex/blob/main/codex-rs/core/src/client_common.rs#L20), [review_prompt.md](https://github.com/openai/codex/blob/main/codex-rs/core/review_prompt.md#L1)).

Together, these systems ensure the agent rebuilds conversation context every turn, monitors usage against model capacities, compacts transcripts when thresholds are exceeded, and tailors its instruction set to the active model or task.
